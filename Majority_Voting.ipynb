{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm,metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of a confusion matrix\n",
    "def perf_measure(y_actual, y_pred):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_pred)): \n",
    "        if y_actual[i]==y_pred[i]==1:\n",
    "           TP += 1\n",
    "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_pred[i]==3:\n",
    "           TN += 1\n",
    "        if y_pred[i]==3 and y_actual[i]!=y_pred[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FN, FP, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON to dataframe and Dataframe to List\n",
    "df1 = pd.read_json('Your File address/News_Vectors.json', encoding='utf-8')\n",
    "veri1 = df1.values.tolist()\n",
    "df2 = pd.read_json('Your File address/fasttext_output.json', encoding='utf-8')\n",
    "veri2 = df2.values.tolist()\n",
    "df3 = pd.read_json('Your File address/tfidf_vector1500.json', encoding='utf-8')\n",
    "veri3 = df3.values.tolist()\n",
    "df0 = pd.read_json('Your File address/etiketli_bistverisi.json', encoding='utf-8')\n",
    "veri0 = df0.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer vector and bist datas to vector and bist lists from datasets\n",
    "vektor1=[]\n",
    "vektor2=[]\n",
    "vektor3=[]\n",
    "bist=[]\n",
    "i=0\n",
    "j=0\n",
    "hedef=len(veri0)\n",
    "while i<hedef:\n",
    "    if(veri0[j][0]!=2):\n",
    "        vektor1.append(veri1[i][1])\n",
    "        vektor2.append(veri2[i][1])\n",
    "        vektor3.append(veri3[i][1])\n",
    "        if j>=hedef-0:\n",
    "            j=hedef-1\n",
    "        bist.append(veri0[j][0])\n",
    "    i+=1\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectors' sizes\n",
    "print(len(vektor1[1]),len(vektor2[1]),len(vektor3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "vektor1, bist, test_size=0.2, random_state=None, shuffle=False)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "vektor2, bist, test_size=0.2, random_state=None, shuffle=False)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(\n",
    "vektor3, bist, test_size=0.2, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = GaussianNB()\n",
    "clf12 = GaussianNB()\n",
    "clf13 = GaussianNB()\n",
    "clf2 = svm.SVC(kernel='linear')\n",
    "clf22 = svm.SVC(kernel='linear')\n",
    "clf23 = svm.SVC(kernel='linear')\n",
    "clf3 = MLPClassifier(hidden_layer_sizes=(100,20,10), activation='relu', solver='adam', max_iter=5000)\n",
    "clf32 = MLPClassifier(hidden_layer_sizes=(100,20,10), activation='relu', solver='adam', max_iter=5000)\n",
    "clf33 = MLPClassifier(hidden_layer_sizes=(100,20,10), activation='relu', solver='adam', max_iter=5000)\n",
    "\n",
    "clf1 = clf1.fit(X_train1, y_train1)\n",
    "clf12 = clf12.fit(X_train2, y_train2)\n",
    "clf13 = clf13.fit(X_train3, y_train3)\n",
    "clf2 = clf2.fit(X_train1, y_train1)\n",
    "clf22 = clf22.fit(X_train2, y_train2)\n",
    "clf23 = clf23.fit(X_train3, y_train3)\n",
    "clf3 = clf3.fit(X_train1, y_train1)\n",
    "clf32 = clf32.fit(X_train2, y_train2)\n",
    "clf33 = clf33.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding of FP and FN datas for 3 models\n",
    "vek1=[]\n",
    "for item, label in zip(X_test1, y_test1):\n",
    "    result = clf3.predict([item])\n",
    "    if result != label:\n",
    "        print (\"predicted label %s, but true label is %s\" % (result, label))\n",
    "        vek1.append(item)\n",
    "vek2=[]\n",
    "for item, label in zip(X_test2, y_test2):\n",
    "    result = clf32.predict([item])\n",
    "    if result != label:\n",
    "        print (\"predicted label %s, but true label is %s\" % (result, label))\n",
    "        vek2.append(item)\n",
    "vek3=[]\n",
    "for item, label in zip(X_test3, y_test3):\n",
    "    result = clf33.predict([item])\n",
    "    if result != label:\n",
    "        print (\"predicted label %s, but true label is %s\" % (result, label))\n",
    "        vek3.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numbers of FP and FN datas for 3 models\n",
    "#for word2vec\n",
    "print(len(vek1))\n",
    "#for fassttext\n",
    "print(len(vek2))\n",
    "#for tf-idf\n",
    "print(len(vek3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding indexes of FP and FN datas for 3 models\n",
    "i=0\n",
    "sayac=0\n",
    "indis1=[]\n",
    "while i<len(vek1):\n",
    "    j=0\n",
    "    while j<len(vektor1):\n",
    "        if(vektor1[j]==vek1[i]):\n",
    "            indis1.append(j)\n",
    "        j+=1\n",
    "    i+=1\n",
    "print(indis1)\n",
    "i=0\n",
    "sayac=0\n",
    "indis2=[]\n",
    "while i<len(vek2):\n",
    "    j=0\n",
    "    while j<len(vektor2):\n",
    "        if(vektor2[j]==vek2[i]):\n",
    "            indis2.append(j)\n",
    "        j+=1\n",
    "    i+=1\n",
    "print(indis2)\n",
    "i=0\n",
    "sayac=0\n",
    "indis3=[]\n",
    "while i<len(vek3):\n",
    "    j=0\n",
    "    while j<len(vektor3):\n",
    "        if(vektor3[j]==vek3[i]):\n",
    "            indis3.append(j)\n",
    "        j+=1\n",
    "    i+=1\n",
    "print(indis3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding of FP and FN datas for tf-idf model\n",
    "#false data can be detected for other models by changing the location of the lists\n",
    "i=0\n",
    "sayac=0\n",
    "indis=[]\n",
    "while i<len(indis3):\n",
    "    j=0\n",
    "    while j<len(indis1):\n",
    "        t=0\n",
    "        while t<len(indis2):\n",
    "            if(indis1[j]!=indis3[i] and indis3[i]!=indis2[t]):\n",
    "                sayac+=1\n",
    "            t+=1\n",
    "        j+=1\n",
    "    if(len(indis1)*len(indis2)==sayac):\n",
    "        indis.append(indis3[i])\n",
    "    sayac=0\n",
    "    i+=1\n",
    "print(sayac)\n",
    "fark=int(((len(vektor1)-1)/100)*80)\n",
    "i=0\n",
    "while(i<len(indis)):\n",
    "    indis[i]=indis[i]-fark\n",
    "    i+=1\n",
    "print(len(indis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding of old f score and old confusion matrix for tf-idf model\n",
    "predict_test=clf33.predict(X_test3)\n",
    "predict_train=clf33.predict(X_train3)\n",
    "print(\"test Accuracy:\",metrics.accuracy_score(y_test3, predict_test))\n",
    "print(\"test f score: \",f1_score(y_test3, predict_test, average='weighted'))\n",
    "print(perf_measure(y_test3, predict_test))\n",
    "print(confusion_matrix(y_test3,predict_test))\n",
    "i=0\n",
    "#convert FP and FN to TN and TP for majority voting\n",
    "while(i<len(indis)): \n",
    "    if y_test3[indis[i]]==1 and y_test3[indis[i]]!=predict_test[indis[i]]:\n",
    "        predict_test[indis[i]]=1\n",
    "    if y_test3[indis[i]]==3 and y_test3[indis[i]]!=predict_test[indis[i]]:\n",
    "        predict_test[indis[i]]=3\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after majority voting, finding of new f-score for tf-idf model\n",
    "print(\"test Accuracy:\",metrics.accuracy_score(y_test3, predict_test))\n",
    "print(\"test f score: \",f1_score(y_test3, predict_test, average='weighted'))\n",
    "#after majority voting, finding of new confusion matrix for tf-idf model\n",
    "print(perf_measure(y_test3, predict_test))\n",
    "print(confusion_matrix(y_test3,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save last models\n",
    "import pickle\n",
    "filename = 'mlp_vot_w2v.sav'\n",
    "pickle.dump(clf3, open(filename, 'wb'))\n",
    "filename = 'mlp_vot_ft.sav'\n",
    "pickle.dump(clf32, open(filename, 'wb'))\n",
    "filename = 'mlp_vot_tfidf.sav'\n",
    "pickle.dump(clf33, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load last models\n",
    "import pickle\n",
    "filename1 = 'mlp_vot_w2v.sav'\n",
    "filename2 = 'mlp_vot_ft.sav'\n",
    "filename3 = 'mlp_vot_tfidf.sav'\n",
    "clf3 = pickle.load(open(filename1, 'rb'))\n",
    "clf32 = pickle.load(open(filename2, 'rb'))\n",
    "clf33 = pickle.load(open(filename3, 'rb'))\n",
    "result = clf3.score(X_test1, y_test1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test1, y_test1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
